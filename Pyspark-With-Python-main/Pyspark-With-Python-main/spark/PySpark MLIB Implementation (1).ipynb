{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "spark_path = 'E:\\\\spark-2.4.3-bin-hadoop2.7'\n",
    "os.environ['SPARK_HOME']= spark_path\n",
    "os.environ['HADOOP_HOME']=spark_path\n",
    "sys.path.append(spark_path+'/bin')\n",
    "sys.path.append(spark_path+'/python')\n",
    "sys.path.append(spark_path+'/python/pyspark')\n",
    "sys.path.append(spark_path+'/python/lib')\n",
    "sys.path.append(spark_path+'/python/lib/pyspark.zip')\n",
    "sys.path.append(spark_path+'/python/lib/py4j-0.10.4-src.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark import SparkConf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to\n",
      "      ____              __\n",
      "     / __/__  ___ _____/ /__\n",
      "    _\\ \\/ _ \\/ _ `/ __/  '_/\n",
      "   /__ / .__/\\_,_/_/ /_/\\_\\   version 2.4.3\n",
      "      /_/\n",
      "\n",
      "Using Python version 3.6.8 (default, Feb 21 2019 18:30:04)\n",
      "SparkSession available as 'spark'.\n"
     ]
    }
   ],
   "source": [
    "# Configure the necessary Spark environment\n",
    "import os\n",
    "import sys\n",
    "\n",
    "pyspark_submit_args = os.environ.get(\"PYSPARK_SUBMIT_ARGS\", \"\")\n",
    "if not \"pyspark-shell\" in pyspark_submit_args: pyspark_submit_args += \" pyspark-shell\"\n",
    "os.environ[\"PYSPARK_SUBMIT_ARGS\"] = pyspark_submit_args\n",
    "\n",
    "spark_home = os.environ.get('SPARK_HOME', None)\n",
    "sys.path.insert(0, spark_home + \"/python\")\n",
    "\n",
    "# Add the py4j to the path.\n",
    "# You may need to change the version number to match your install\n",
    "sys.path.insert(0, os.path.join(spark_home, \"python/lib/py4j-0.10.4-src.zip\"))\n",
    "\n",
    "# Initialize PySpark\n",
    "exec(open(os.path.join(spark_home, \"python/pyspark/shell.py\")).read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: py4j in f:\\users\\vignesh\\anaconda3\\envs\\keras_tf\\lib\\site-packages (0.10.7)\n"
     ]
    }
   ],
   "source": [
    "!pip install py4j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = SparkContext(master='local')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://DESKTOP-FU1FQEQ.Dlink:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.3</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local appName=pyspark-shell>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.types import *\n",
    "sqlContext = SQLContext(sc)\n",
    "\n",
    "rdd = sc.parallelize([('X01','2014-02-13T12:36:14.899','2014-02-13T12:31:56.876','sip:4534454450'),\n",
    "                                    ('X02','2014-02-13T12:35:37.405','2014-02-13T12:32:13.321','sip:6413445440'),\n",
    "                                    ('X03','2014-02-13T12:36:03.825','2014-02-13T12:32:15.229','sip:4534437492'),\n",
    "                                    ('XO4','2014-02-13T12:37:05.460','2014-02-13T12:32:36.881','sip:6474454453'),\n",
    "                                    ('XO5','2014-02-13T12:36:52.721','2014-02-13T12:33:30.323','sip:8874458555')])\n",
    "schema = StructType([StructField('ID', StringType(), True),\n",
    "                     StructField('EndDateTime', StringType(), True),\n",
    "                     StructField('StartDateTime', StringType(), True)])\n",
    "df = sqlContext.createDataFrame(rdd, schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'setCallSite'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-25-c807640ba7bf>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mF:\\Users\\Vignesh\\Anaconda3\\envs\\Keras_TF\\lib\\site-packages\\pyspark\\sql\\dataframe.py\u001b[0m in \u001b[0;36mtake\u001b[1;34m(self, num)\u001b[0m\n\u001b[0;32m    569\u001b[0m         \u001b[1;33m[\u001b[0m\u001b[0mRow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mage\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34mu'Alice'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mage\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34mu'Bob'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    570\u001b[0m         \"\"\"\n\u001b[1;32m--> 571\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlimit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    572\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    573\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0msince\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1.3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mF:\\Users\\Vignesh\\Anaconda3\\envs\\Keras_TF\\lib\\site-packages\\pyspark\\sql\\dataframe.py\u001b[0m in \u001b[0;36mcollect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    530\u001b[0m         \u001b[1;33m[\u001b[0m\u001b[0mRow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mage\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34mu'Alice'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mage\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34mu'Bob'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    531\u001b[0m         \"\"\"\n\u001b[1;32m--> 532\u001b[1;33m         \u001b[1;32mwith\u001b[0m \u001b[0mSCCallSiteSync\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mcss\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    533\u001b[0m             \u001b[0msock_info\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollectToPython\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    534\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msock_info\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mBatchedSerializer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mPickleSerializer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mF:\\Users\\Vignesh\\Anaconda3\\envs\\Keras_TF\\lib\\site-packages\\pyspark\\traceback_utils.py\u001b[0m in \u001b[0;36m__enter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     70\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__enter__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     71\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mSCCallSiteSync\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_spark_stack_depth\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 72\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_context\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jsc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msetCallSite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_site\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     73\u001b[0m         \u001b[0mSCCallSiteSync\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_spark_stack_depth\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'setCallSite'"
     ]
    }
   ],
   "source": [
    "df.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ID: string (nullable = true)\n",
      " |-- EndDateTime: string (nullable = true)\n",
      " |-- StartDateTime: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o189.describe.\n: java.lang.IllegalStateException: SparkContext has been shutdown\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2053)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2126)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:945)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\r\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:944)\r\n\tat org.apache.spark.sql.execution.stat.StatFunctions$.aggResult$lzycompute$1(StatFunctions.scala:273)\r\n\tat org.apache.spark.sql.execution.stat.StatFunctions$.org$apache$spark$sql$execution$stat$StatFunctions$$aggResult$1(StatFunctions.scala:273)\r\n\tat org.apache.spark.sql.execution.stat.StatFunctions$$anonfun$summary$2.apply$mcVI$sp(StatFunctions.scala:286)\r\n\tat scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:160)\r\n\tat org.apache.spark.sql.execution.stat.StatFunctions$.summary(StatFunctions.scala:285)\r\n\tat org.apache.spark.sql.Dataset.summary(Dataset.scala:2533)\r\n\tat org.apache.spark.sql.Dataset.describe(Dataset.scala:2472)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\r\n\tat java.lang.reflect.Method.invoke(Unknown Source)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.lang.Thread.run(Unknown Source)\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-29-4a7106bdf59c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdescribe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoPandas\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mF:\\Users\\Vignesh\\Anaconda3\\envs\\Keras_TF\\lib\\site-packages\\pyspark\\sql\\dataframe.py\u001b[0m in \u001b[0;36mdescribe\u001b[1;34m(self, *cols)\u001b[0m\n\u001b[0;32m   1170\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcols\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcols\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1171\u001b[0m             \u001b[0mcols\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcols\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1172\u001b[1;33m         \u001b[0mjdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdescribe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jseq\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcols\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1173\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjdf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msql_ctx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1174\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mF:\\Users\\Vignesh\\Anaconda3\\envs\\Keras_TF\\lib\\site-packages\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[1;32m-> 1257\u001b[1;33m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[0;32m   1258\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1259\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mF:\\Users\\Vignesh\\Anaconda3\\envs\\Keras_TF\\lib\\site-packages\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m     61\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mF:\\Users\\Vignesh\\Anaconda3\\envs\\Keras_TF\\lib\\site-packages\\py4j\\protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[0;32m    327\u001b[0m                     \u001b[1;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 328\u001b[1;33m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[0;32m    329\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o189.describe.\n: java.lang.IllegalStateException: SparkContext has been shutdown\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2053)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2126)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:945)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\r\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:944)\r\n\tat org.apache.spark.sql.execution.stat.StatFunctions$.aggResult$lzycompute$1(StatFunctions.scala:273)\r\n\tat org.apache.spark.sql.execution.stat.StatFunctions$.org$apache$spark$sql$execution$stat$StatFunctions$$aggResult$1(StatFunctions.scala:273)\r\n\tat org.apache.spark.sql.execution.stat.StatFunctions$$anonfun$summary$2.apply$mcVI$sp(StatFunctions.scala:286)\r\n\tat scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:160)\r\n\tat org.apache.spark.sql.execution.stat.StatFunctions$.summary(StatFunctions.scala:285)\r\n\tat org.apache.spark.sql.Dataset.summary(Dataset.scala:2533)\r\n\tat org.apache.spark.sql.Dataset.describe(Dataset.scala:2472)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\r\n\tat java.lang.reflect.Method.invoke(Unknown Source)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.lang.Thread.run(Unknown Source)\r\n"
     ]
    }
   ],
   "source": [
    "df.describe().toPandas().transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql import SQLContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()\n",
    "sc= SparkContext()\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(CRIM=0.00632, ZN=18.0, INDUS=2.309999943, CHAS=0, NOX=0.537999988, RM=6.574999809, AGE=65.19999695, DIS=4.090000153, RAD=1, TAX=296, PT=15.30000019, B=396.8999939, LSTAT=4.980000019, MV=24.0),\n",
       " Row(CRIM=0.027310001, ZN=0.0, INDUS=7.070000172, CHAS=0, NOX=0.469000012, RM=6.421000004, AGE=78.90000153, DIS=4.967100143, RAD=2, TAX=242, PT=17.79999924, B=396.8999939, LSTAT=9.140000343, MV=21.60000038),\n",
       " Row(CRIM=0.02729, ZN=0.0, INDUS=7.070000172, CHAS=0, NOX=0.469000012, RM=7.184999943, AGE=61.09999847, DIS=4.967100143, RAD=2, TAX=242, PT=17.79999924, B=392.8299866, LSTAT=4.03000021, MV=34.70000076),\n",
       " Row(CRIM=0.032370001, ZN=0.0, INDUS=2.180000067, CHAS=0, NOX=0.458000004, RM=6.998000145, AGE=45.79999924, DIS=6.062200069, RAD=3, TAX=222, PT=18.70000076, B=394.6300049, LSTAT=2.940000057, MV=33.40000153),\n",
       " Row(CRIM=0.069049999, ZN=0.0, INDUS=2.180000067, CHAS=0, NOX=0.458000004, RM=7.146999836, AGE=54.20000076, DIS=6.062200069, RAD=3, TAX=222, PT=18.70000076, B=396.8999939, LSTAT=5.329999924, MV=36.20000076)]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "house_df = sqlContext.read.format('com.databricks.spark.csv').options(header='true', inferschema='true').load('C:/Users/Vignesh/Downloads/boston.csv')\n",
    "house_df.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[CRIM: double, ZN: double, INDUS: double, CHAS: int, NOX: double, RM: double, AGE: double, DIS: double, RAD: int, TAX: int, PT: double, B: double, LSTAT: double, MV: double]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "house_df.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- CRIM: double (nullable = true)\n",
      " |-- ZN: double (nullable = true)\n",
      " |-- INDUS: double (nullable = true)\n",
      " |-- CHAS: integer (nullable = true)\n",
      " |-- NOX: double (nullable = true)\n",
      " |-- RM: double (nullable = true)\n",
      " |-- AGE: double (nullable = true)\n",
      " |-- DIS: double (nullable = true)\n",
      " |-- RAD: integer (nullable = true)\n",
      " |-- TAX: integer (nullable = true)\n",
      " |-- PT: double (nullable = true)\n",
      " |-- B: double (nullable = true)\n",
      " |-- LSTAT: double (nullable = true)\n",
      " |-- MV: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "house_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Perform descriptive analytics\n",
    "#!pip install pandas\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>summary</th>\n",
       "      <th>CRIM</th>\n",
       "      <th>ZN</th>\n",
       "      <th>INDUS</th>\n",
       "      <th>CHAS</th>\n",
       "      <th>NOX</th>\n",
       "      <th>RM</th>\n",
       "      <th>AGE</th>\n",
       "      <th>DIS</th>\n",
       "      <th>RAD</th>\n",
       "      <th>TAX</th>\n",
       "      <th>PT</th>\n",
       "      <th>B</th>\n",
       "      <th>LSTAT</th>\n",
       "      <th>MV</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>count</td>\n",
       "      <td>506</td>\n",
       "      <td>506</td>\n",
       "      <td>506</td>\n",
       "      <td>506</td>\n",
       "      <td>506</td>\n",
       "      <td>506</td>\n",
       "      <td>506</td>\n",
       "      <td>506</td>\n",
       "      <td>506</td>\n",
       "      <td>506</td>\n",
       "      <td>506</td>\n",
       "      <td>506</td>\n",
       "      <td>506</td>\n",
       "      <td>506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>mean</td>\n",
       "      <td>3.6135235608162057</td>\n",
       "      <td>11.363636363636363</td>\n",
       "      <td>11.136778749531626</td>\n",
       "      <td>0.0691699604743083</td>\n",
       "      <td>0.5546950602312246</td>\n",
       "      <td>6.28463438896641</td>\n",
       "      <td>68.57490120115612</td>\n",
       "      <td>3.7950426960059325</td>\n",
       "      <td>9.549407114624506</td>\n",
       "      <td>408.2371541501976</td>\n",
       "      <td>18.45553382776679</td>\n",
       "      <td>356.67402960597883</td>\n",
       "      <td>12.653063233922925</td>\n",
       "      <td>22.53280636250988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>stddev</td>\n",
       "      <td>8.601545086715594</td>\n",
       "      <td>23.32245299451514</td>\n",
       "      <td>6.86035298095724</td>\n",
       "      <td>0.2539940413404101</td>\n",
       "      <td>0.1158776754570543</td>\n",
       "      <td>0.7026171549511354</td>\n",
       "      <td>28.148861532793276</td>\n",
       "      <td>2.105710142043288</td>\n",
       "      <td>8.707259384239366</td>\n",
       "      <td>168.53711605495903</td>\n",
       "      <td>2.164945780039869</td>\n",
       "      <td>91.29486340272308</td>\n",
       "      <td>7.141061500195388</td>\n",
       "      <td>9.197104107945272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>min</td>\n",
       "      <td>0.00632</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.460000008</td>\n",
       "      <td>0</td>\n",
       "      <td>0.38499999</td>\n",
       "      <td>3.561000109</td>\n",
       "      <td>2.900000095</td>\n",
       "      <td>1.129600048</td>\n",
       "      <td>1</td>\n",
       "      <td>187</td>\n",
       "      <td>12.60000038</td>\n",
       "      <td>0.319999993</td>\n",
       "      <td>1.730000019</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>max</td>\n",
       "      <td>88.97619629</td>\n",
       "      <td>100.0</td>\n",
       "      <td>27.73999977</td>\n",
       "      <td>1</td>\n",
       "      <td>0.870999992</td>\n",
       "      <td>8.779999733</td>\n",
       "      <td>100.0</td>\n",
       "      <td>12.12650013</td>\n",
       "      <td>24</td>\n",
       "      <td>711</td>\n",
       "      <td>22.0</td>\n",
       "      <td>396.8999939</td>\n",
       "      <td>37.97000122</td>\n",
       "      <td>50.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  summary                CRIM                  ZN               INDUS  \\\n",
       "0   count                 506                 506                 506   \n",
       "1    mean  3.6135235608162057  11.363636363636363  11.136778749531626   \n",
       "2  stddev   8.601545086715594   23.32245299451514    6.86035298095724   \n",
       "3     min             0.00632                 0.0         0.460000008   \n",
       "4     max         88.97619629               100.0         27.73999977   \n",
       "\n",
       "                 CHAS                 NOX                  RM  \\\n",
       "0                 506                 506                 506   \n",
       "1  0.0691699604743083  0.5546950602312246    6.28463438896641   \n",
       "2  0.2539940413404101  0.1158776754570543  0.7026171549511354   \n",
       "3                   0          0.38499999         3.561000109   \n",
       "4                   1         0.870999992         8.779999733   \n",
       "\n",
       "                  AGE                 DIS                RAD  \\\n",
       "0                 506                 506                506   \n",
       "1   68.57490120115612  3.7950426960059325  9.549407114624506   \n",
       "2  28.148861532793276   2.105710142043288  8.707259384239366   \n",
       "3         2.900000095         1.129600048                  1   \n",
       "4               100.0         12.12650013                 24   \n",
       "\n",
       "                  TAX                 PT                   B  \\\n",
       "0                 506                506                 506   \n",
       "1   408.2371541501976  18.45553382776679  356.67402960597883   \n",
       "2  168.53711605495903  2.164945780039869   91.29486340272308   \n",
       "3                 187        12.60000038         0.319999993   \n",
       "4                 711               22.0         396.8999939   \n",
       "\n",
       "                LSTAT                 MV  \n",
       "0                 506                506  \n",
       "1  12.653063233922925  22.53280636250988  \n",
       "2   7.141061500195388  9.197104107945272  \n",
       "3         1.730000019                5.0  \n",
       "4         37.97000122               50.0  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "house_df.describe().toPandas().transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scatter matrix is a great way to roughly determine if we have a linear correlation between multiple independent variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas.plotting import scatter_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "numeric_features = [t[0] for t in house_df.dtypes if t[1] == 'int' or t[1] == 'double']\n",
    "sampled_data = house_df.select(numeric_features).sample(False, 0.8).toPandas()\n",
    "axs = scatter_matrix(sampled_data, figsize=(10, 10))\n",
    "n = len(sampled_data.columns)\n",
    "for i in range(n):\n",
    "    v = axs[i, 0]\n",
    "    v.yaxis.label.set_rotation(0)\n",
    "    v.yaxis.label.set_ha('right')\n",
    "    v.set_yticks(())\n",
    "    h = axs[n-1, i]\n",
    "    h.xaxis.label.set_rotation(90)\n",
    "    h.set_xticks(())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Correlation to MV for ', 'CRIM', -0.3883046116575088)\n",
      "('Correlation to MV for ', 'ZN', 0.36044534463752903)\n",
      "('Correlation to MV for ', 'INDUS', -0.48372517128143383)\n",
      "('Correlation to MV for ', 'CHAS', 0.17526017775291847)\n",
      "('Correlation to MV for ', 'NOX', -0.4273207763683772)\n",
      "('Correlation to MV for ', 'RM', 0.695359937127267)\n",
      "('Correlation to MV for ', 'AGE', -0.37695456714288667)\n",
      "('Correlation to MV for ', 'DIS', 0.24992873873512172)\n",
      "('Correlation to MV for ', 'RAD', -0.3816262315669168)\n",
      "('Correlation to MV for ', 'TAX', -0.46853593528654536)\n",
      "('Correlation to MV for ', 'PT', -0.5077867038116085)\n",
      "('Correlation to MV for ', 'B', 0.3334608226834164)\n",
      "('Correlation to MV for ', 'LSTAT', -0.7376627294671615)\n",
      "('Correlation to MV for ', 'MV', 1.0)\n"
     ]
    }
   ],
   "source": [
    "import six\n",
    "for i in house_df.columns:\n",
    "    if not( isinstance(house_df.select(i).take(1)[0][0], six.string_types)):\n",
    "        print( \"Correlation to MV for \", i, house_df.stat.corr('MV',i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----------+\n",
      "|            features|         MV|\n",
      "+--------------------+-----------+\n",
      "|[0.00632,18.0,2.3...|       24.0|\n",
      "|[0.027310001,0.0,...|21.60000038|\n",
      "|[0.02729,0.0,7.07...|34.70000076|\n",
      "+--------------------+-----------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "vectorAssembler = VectorAssembler(inputCols = ['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PT', 'B', 'LSTAT'], outputCol = 'features')\n",
    "vhouse_df = vectorAssembler.transform(house_df)\n",
    "vhouse_df = vhouse_df.select(['features', 'MV'])\n",
    "vhouse_df.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "splits = vhouse_df.randomSplit([0.7, 0.3])\n",
    "train_df = splits[0]\n",
    "test_df = splits[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients: [-0.049687906711926716,0.005729138144892694,0.0,1.2930551983265361,-5.127862706125868,3.7946667898241113,0.0,-0.6235890579389267,0.012500702919514804,0.0,-0.7678424543743334,0.007914622611657924,-0.524573600080142]\n",
      "Intercept: 21.6469319588\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.regression import LinearRegression\n",
    "lr = LinearRegression(featuresCol = 'features', labelCol='MV', maxIter=10, regParam=0.3, elasticNetParam=0.8)\n",
    "lr_model = lr.fit(train_df)\n",
    "print(\"Coefficients: \" + str(lr_model.coefficients))\n",
    "print(\"Intercept: \" + str(lr_model.intercept))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 4.960349\n",
      "r2: 0.689697\n"
     ]
    }
   ],
   "source": [
    "#Summarize the model over the training set and print out some metrics:\n",
    "trainingSummary = lr_model.summary\n",
    "print(\"RMSE: %f\" % trainingSummary.rootMeanSquaredError)\n",
    "print(\"r2: %f\" % trainingSummary.r2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+\n",
      "|summary|                MV|\n",
      "+-------+------------------+\n",
      "|  count|               340|\n",
      "|   mean|22.372647093376475|\n",
      "| stddev| 8.917818137541282|\n",
      "|    min|               5.0|\n",
      "|    max|              50.0|\n",
      "+-------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_df.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-----------+--------------------+\n",
      "|        prediction|         MV|            features|\n",
      "+------------------+-----------+--------------------+\n",
      "|31.023967272205553|32.70000076|[0.01301,35.0,1.5...|\n",
      "| 36.85660935784797|       50.0|[0.01381,80.0,0.4...|\n",
      "|25.719303860488395|30.10000038|[0.01709,90.0,2.0...|\n",
      "|25.463692979697388|       33.0|[0.019509999,17.5...|\n",
      "|20.061573817818797|20.10000038|[0.019649999,80.0...|\n",
      "|37.694326498812316|       50.0|[0.020090001,95.0...|\n",
      "|27.236851147822602|23.89999962|[0.025429999,55.0...|\n",
      "|28.732248796121993|31.20000076|[0.03049,55.0,3.7...|\n",
      "| 31.24440925650234|34.90000153|[0.03359,75.0,2.9...|\n",
      "|29.542372859430287|       28.5|[0.035020001,80.0...|\n",
      "|25.227220988202323|24.79999924|[0.036589999,25.0...|\n",
      "|27.589869673450398|       28.0|[0.041129999,25.0...|\n",
      "| 31.08000952169187|30.29999924|[0.046659999,80.0...|\n",
      "|22.921544529947212|11.89999962|[0.04741,0.0,11.9...|\n",
      "|28.568646008585084|28.20000076|[0.049320001,33.0...|\n",
      "+------------------+-----------+--------------------+\n",
      "only showing top 15 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#make predictions\n",
    "lr_predictions = lr_model.transform(test_df)\n",
    "lr_predictions.select(\"prediction\",\"MV\",\"features\").show(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R Squared (R2) on test data = 0.738979\n"
     ]
    }
   ],
   "source": [
    "#evaluate results\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "lr_evaluator = RegressionEvaluator(predictionCol=\"prediction\", \\\n",
    "                 labelCol=\"MV\",metricName=\"r2\")\n",
    "print(\"R Squared (R2) on test data = %g\" % lr_evaluator.evaluate(lr_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error (RMSE) on test data = 4.97332\n"
     ]
    }
   ],
   "source": [
    "test_result = lr_model.evaluate(test_df)\n",
    "print(\"Root Mean Squared Error (RMSE) on test data = %g\" % test_result.rootMeanSquaredError)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numIterations: 11\n",
      "objectiveHistory: [0.49999999999999956, 0.4357753677704518, 0.25185366664668424, 0.22804458277677025, 0.19937888801402628, 0.19553404353871723, 0.1945958474105226, 0.1934565829277708, 0.19293755562375098, 0.19234933157374726, 0.19205062722890984]\n",
      "+-------------------+\n",
      "|          residuals|\n",
      "+-------------------+\n",
      "| -6.183854552431036|\n",
      "| 2.4785283655311474|\n",
      "| -4.968314429966092|\n",
      "|  6.291630586031637|\n",
      "| 1.9343180097566695|\n",
      "|  1.309551303217436|\n",
      "|-0.7331612773792031|\n",
      "|-2.2969128386984323|\n",
      "| 11.163504584036467|\n",
      "| 10.269145335110856|\n",
      "| 2.7824242630052503|\n",
      "|-2.3119245802313806|\n",
      "|-0.6783855132152041|\n",
      "| 7.1064695161425675|\n",
      "| 0.7324915885003307|\n",
      "| -9.751238887723392|\n",
      "|  3.939665407620687|\n",
      "|-3.6128501882489203|\n",
      "|  2.410630411400426|\n",
      "|-1.6637210062722616|\n",
      "+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#we achieved worse RMSE and R squared on the test set.\n",
    "\n",
    "print(\"numIterations: %d\" % trainingSummary.totalIterations)\n",
    "print(\"objectiveHistory: %s\" % str(trainingSummary.objectiveHistory))\n",
    "trainingSummary.residuals.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-----------+--------------------+\n",
      "|        prediction|         MV|            features|\n",
      "+------------------+-----------+--------------------+\n",
      "| 37.77147779908827|       50.0|[0.01381,80.0,0.4...|\n",
      "| 25.65319880049389|30.10000038|[0.01709,90.0,2.0...|\n",
      "| 31.11112296372059|32.90000153|[0.01778,95.0,1.4...|\n",
      "| 26.05082059769446|23.10000038|[0.0187,85.0,4.15...|\n",
      "| 38.72157510322816|       50.0|[0.020090001,95.0...|\n",
      "|26.236732285628218|       16.5|[0.024979999,0.0,...|\n",
      "|  27.9457409166711|23.89999962|[0.025429999,55.0...|\n",
      "|20.231784941037567|       17.5|[0.031129999,0.0,...|\n",
      "|28.872751441743098|24.10000038|[0.034449998,82.5...|\n",
      "| 24.75384880004313|19.39999962|[0.03466,35.0,6.0...|\n",
      "|  36.1524494473268|45.40000153|[0.035780001,20.0...|\n",
      "| 29.49920138186634|       23.5|[0.035840001,80.0...|\n",
      "|26.015594237724947|24.79999924|[0.036589999,25.0...|\n",
      "| 27.56217628539805|       22.0|[0.03932,0.0,3.41...|\n",
      "|35.181479893967605|33.29999924|[0.040109999,80.0...|\n",
      "|24.793055190918338|19.39999962|[0.043790001,80.0...|\n",
      "|24.568151445062487|19.79999924|[0.04544,0.0,3.24...|\n",
      "|23.335562383314407|11.89999962|[0.04741,0.0,11.9...|\n",
      "|29.735596995645903|28.20000076|[0.049320001,33.0...|\n",
      "|23.567114498008507|22.20000076|[0.050829999,0.0,...|\n",
      "+------------------+-----------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Using our Linear Regression model to make some predictions\n",
    "predictions = lr_model.transform(test_df)\n",
    "predictions.select(\"prediction\",\"MV\",\"features\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision tree for Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeRegressor_4b16ae1397359c9e2b25"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.regression import DecisionTreeRegressor\n",
    "dt = DecisionTreeRegressor(featuresCol ='features', labelCol = '\n",
    "                           MV')\n",
    "dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create model on the training set\n",
    "dt_model = dt.fit(train_df)\n",
    "\n",
    "# make predictions on the test dataset\n",
    "dt_predictions = dt_model.transform(test_df)\n",
    "\n",
    "# evaluate the prediction and print RMSE\n",
    "dt_evaluator = RegressionEvaluator(\n",
    "    labelCol=\"MV\", predictionCol=\"prediction\", metricName=\"rmse\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error (RMSE) on test data = 4.00338\n"
     ]
    }
   ],
   "source": [
    "rmse = dt_evaluator.evaluate(dt_predictions)\n",
    "print(\"Root Mean Squared Error (RMSE) on test data = %g\" % rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SparseVector(13, {0: 0.0424, 4: 0.0059, 5: 0.3019, 7: 0.0823, 9: 0.0045, 10: 0.0304, 11: 0.005, 12: 0.5276})"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Feature Importance\n",
    "dt_model.featureImportances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(CRIM=0.00632, ZN=18.0, INDUS=2.309999943, CHAS=0, NOX=0.537999988, RM=6.574999809, AGE=65.19999695, DIS=4.090000153, RAD=1, TAX=296, PT=15.30000019, B=396.8999939, LSTAT=4.980000019, MV=24.0),\n",
       " Row(CRIM=0.027310001, ZN=0.0, INDUS=7.070000172, CHAS=0, NOX=0.469000012, RM=6.421000004, AGE=78.90000153, DIS=4.967100143, RAD=2, TAX=242, PT=17.79999924, B=396.8999939, LSTAT=9.140000343, MV=21.60000038),\n",
       " Row(CRIM=0.02729, ZN=0.0, INDUS=7.070000172, CHAS=0, NOX=0.469000012, RM=7.184999943, AGE=61.09999847, DIS=4.967100143, RAD=2, TAX=242, PT=17.79999924, B=392.8299866, LSTAT=4.03000021, MV=34.70000076)]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "house_df.take(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient-boosted tree regression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-----------+--------------------+\n",
      "|        prediction|         MV|            features|\n",
      "+------------------+-----------+--------------------+\n",
      "|33.340501037360035|32.70000076|[0.01301,35.0,1.5...|\n",
      "|47.790157452082816|       50.0|[0.01381,80.0,0.4...|\n",
      "|29.230305993053125|30.10000038|[0.01709,90.0,2.0...|\n",
      "|  32.1976668947514|       33.0|[0.019509999,17.5...|\n",
      "| 19.20520401324379|20.10000038|[0.019649999,80.0...|\n",
      "+------------------+-----------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# import library\n",
    "from pyspark.ml.regression import GBTRegressor\n",
    "\n",
    "# initialize the model and declare the features and the target column\n",
    "gbt = GBTRegressor(featuresCol = 'features', labelCol = 'MV', maxIter=10)\n",
    "\n",
    "# train model\n",
    "gbt_model = gbt.fit(train_df)\n",
    "\n",
    "# make predictions\n",
    "gbt_predictions = gbt_model.transform(test_df)\n",
    "\n",
    "# show actual and predicted columns with the features\n",
    "gbt_predictions.select('prediction', 'MV', 'features').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error (RMSE) on test data = 3.6736\n"
     ]
    }
   ],
   "source": [
    "# evaluate the model and print RMSE\n",
    "gbt_evaluator = RegressionEvaluator(\n",
    "    labelCol=\"MV\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "\n",
    "# print RMSE\n",
    "rmse = gbt_evaluator.evaluate(gbt_predictions)\n",
    "\n",
    "print(\"Root Mean Squared Error (RMSE) on test data = %g\" % rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gradient-boosted tree regression performed the best on our data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
